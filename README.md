# ğŸ¤– GenAI Output Validation

### ğŸ’¡ Problem

You've built/evaluating your **own custom LLM** (due to data privacy or confidentiality reasons), or you want to evaluate the performance of a new **LLM of your interest** against a **mature LLM** such as **ChatGPT**, **Gemini**, **LLaMA**, etc.

**Validating AI output** means measuring how accurate, relevant, or reliable an **AI-generated response** is for a given input prompt (from your LLM of interest), compared to a reference or ideal answer.

Using a `JudgeLLM` (a **mature LLM** that is widely accepted, such as ChatGPT, Gemini, LLaMA, etc.) allows you to automate this evaluation.

---

### ğŸ“Š About Quantitative Validation

**Quantitative validation** includes:
- ğŸ” **Comparison** against a reference or ground-truth answer.  
- ğŸ§® **Scoring** AI-generated outputs based on measurable quality metrics.  
- ğŸ“ˆ **Aggregation** of scores to produce an overall performance rating.  
- ğŸ§  **Reasoned Justification** explaining why a particular score or judgment was given.

---

### âš™ï¸ğŸ› ï¸ Solution

```mermaid
flowchart TD

    %% -----------------------------
    %% Core Test Case Structure
    %% -----------------------------
    subgraph TC["LLM Test Case"]
        P["ğŸ§© input_prompt"]
        EA["ğŸ“˜ expected_answer"]
        AGA["ğŸ¤– ai_generated_answer"]
        SoT["ğŸ“„ source_of_truth"]
        AiT["ğŸ§  ai_inferred_truth"]
    end

    %% -----------------------------
    %% Metrics Section
    %% -----------------------------
    subgraph METRICS["GenAI QA Framework"]
        AR["Answer Relevancy"]
        FA["Faithfulness"]
        CP["Contextual Precision"]
        CR["Contextual Recall"]
        CRe["Contextual Relevancy"]
        HA["Hallucination"]
        SU["Summarization"]
    end

    %% -----------------------------
    %% Judge LLM
    %% -----------------------------
    subgraph LLM["Judge LLM"]
        J["AmazonBedrockModel (amazon.nova-pro-v1:0)"]
    end

    %% -----------------------------
    %% Main Flow
    %% -----------------------------
    TC -->|"Passed to"| METRICS
    METRICS -->|"Each metric uses"| LLM
    LLM -->|"Evaluates each test metric for a given LLM Test Case"| METRICS

    %% -----------------------------
    %% Relationships and Outputs
    %% -----------------------------
    METRICS -->|"Generate test scores + reasons"| RES["ğŸ“Š Metric Results per Test Case"]
    RES -->|"Aggregated across test cases"| AVG["ğŸ“ˆ Aggregated Test Metrics"]

    %% -----------------------------
    %% Visual Group Labels
    %% -----------------------------
    classDef main fill:#f9f9f9,stroke:#333,stroke-width:1px;
    classDef block fill:#eef6ff,stroke:#4a90e2,stroke-width:1px;
    classDef judge fill:#fff2cc,stroke:#e6b800,stroke-width:1px;
    classDef result fill:#e6ffed,stroke:#27ae60,stroke-width:1px;

    class TC,SoT,AiT,P,EA,AGA block
    class METRICS,AR,FA,CP,CR,CRe,HA,SU main
    class LLM,J judge
    class RES,AVG result
```

---

### ğŸ§© LLM Test Case Structure

Each test case contains the following fields:

| **Key**               | **Type** | **Required** | **Description** |
|------------------------|-----------|---------------|------------------|
| `input_prompt`         | `str`     | âœ… | The question or instruction given to the AI model. |
| `expected_answer`      | `str`     | âœ… | The correct or reference answer (ground truth). |
| `ai_generated_answer`  | `str`     | âœ… | The actual response produced by the AI model. |
| `test_case_group`      | `str`     | âŒ | Optional logical grouping (e.g., â€œGeneral Knowledgeâ€, â€œMathâ€, â€œGeographyâ€). Defaults to `"default"`. |
| `test_case_name`       | `str`     | âŒ | Optional unique name for the test case. Defaults to the `input_prompt` value. |
| `background_context`   | `str`     | âŒ | Optional textual context or reference document supporting the expected answer. If not provided, it is auto-generated as a combination of `input_prompt` and `expected_answer`. |
| `ai_inferred_context`  | `str`     | âŒ | Optional textual context inferred from the AIâ€™s reasoning. If not provided, it is auto-generated as a combination of `input_prompt` and `ai_generated_answer`. |


---

### ğŸ§® **GenAI Test Metrics**

All metrics produce a score between **0.0** and **1.0**, where **1.0** ğŸ¯ indicates perfect alignment with the evaluation goal.  
Each metric includes a **ğŸ’¬ Reason** to explain the score â€” showing what the AI did well âœ… or where it failed âŒ.

#### 1ï¸âƒ£ ğŸ’¬ **Answer Relevancy**
- **ğŸ¯ Purpose:** Measures how relevant the AI-generated answer is to the original input prompt.  
- **ğŸ“Š Scoring:** `0.0 â€“ 1.0`  
- **ğŸ§  Reason:** Explains whether the output directly addresses the user query.  
  ğŸ”» Low scores = off-topic or partially relevant answers.

#### 2ï¸âƒ£ ğŸ“š **Faithfulness**
- **ğŸ¯ Purpose:** Assesses whether the AI output is factually correct âœ… relative to the source/reference.  
- **ğŸ“Š Scoring:** `0.0 â€“ 1.0`  
- **ğŸ” Reason:** Highlights factual errors âŒ or unsupported claims.  
  ğŸ’¯ High scores = answer faithfully reflects expected content.

#### 3ï¸âƒ£ ğŸ¯ **Contextual Precision**
- **ğŸ¯ Purpose:** Measures the proportion of relevant content in the AI output compared to all content generated.  
- **ğŸ“Š Scoring:** `0.0 â€“ 1.0`  
- **ğŸ” Reason:** Indicates if irrelevant or extra content was produced.  
  âš ï¸ Low precision = unnecessary or inaccurate info included.

#### 4ï¸âƒ£ ğŸ“ˆ **Contextual Recall**
- **ğŸ¯ Purpose:** Measures how much of the expected information is actually present in the AI output.  
- **ğŸ“Š Scoring:** `0.0 â€“ 1.0`  
- **ğŸ§¾ Reason:** Explains which key points were missing.  
  ğŸš€ High recall = most expected content covered.

#### 5ï¸âƒ£ ğŸ”— **Contextual Relevancy**
- **ğŸ¯ Purpose:** Combines **Precision** ğŸ¯ and **Recall** ğŸ“ˆ to evaluate how relevant and complete the content is in context.  
- **ğŸ“Š Scoring:** `0.0 â€“ 1.0`  
- **ğŸ§© Reason:** Highlights missing or irrelevant pieces, giving a holistic view ğŸŒ of content quality.

#### 6ï¸âƒ£ ğŸš« **Hallucination**
- **ğŸ¯ Purpose:** Detects whether the AI produced fabricated ğŸŒ€ or unsupported content.  
- **ğŸ“Š Scoring:** `0.0 â€“ 1.0`  
- **ğŸ” Reason:** High score âœ… = no hallucinations;  
  Low score âŒ = identifies speculative or false content.

#### 7ï¸âƒ£ ğŸ“ **Summarization**
- **ğŸ¯ Purpose:** Evaluates how well the AI output summarizes ğŸ“š the provided context or expected answer.  
- **ğŸ“Š Scoring:** `0.0 â€“ 1.0`  
- **ğŸ§  Reason:** Indicates whether key points were retained âœ¨ or lost, and if the summary is concise and coherent ğŸ’¬.

---

## ğŸ§© Pre-requisites
- ğŸ Install **Python3**  
- â˜ï¸ Ensure you have an **AWS account** and valid **AWS credentials** ğŸ”‘  
- âš™ï¸ Configure your `AWS credentials` to your default AWS profile using `aws configure` command inside the `$HOME/.aws` folder ğŸ“‚.

## ğŸš€ How to Run

```
python3 -m venv llm-testing-venv
source llm-testing-venv/bin/activate
pip install -r requirements.txt
python main.py
```

## ğŸ“ Notes
ğŸ§  AWS is not a mandatory requirement to run `deepeval` test cases.
It is ONLY listed as a pre-requisite here since `Amazon-Nova-Pro` is being used as the `JudgeLLM` for this demo (see `modules/judge_llm.py`)


You can choose **any trusted LLM** ğŸ¤– as your `JudgeLLM` from the following list in the `deepeval` library:
```
https://github.com/confident-ai/deepeval/tree/main/deepeval/models/llms
```
ğŸ§  The available options for choosing a `JudgeLLM` (as per the above link) includes:
- ğŸ’¬ OpenAI
- ğŸ” Deepseek
- ğŸŒ Gemini
- â˜ï¸ AWS
- ğŸ”· Azure
- ğŸ§  Ollama
- â• and more...

ğŸ’¡ You can even **create your own custom** `JudgeLLM` ğŸ§© by extending the class: 
`deepeval.models.DeepEvalBaseModel` ğŸ§±
